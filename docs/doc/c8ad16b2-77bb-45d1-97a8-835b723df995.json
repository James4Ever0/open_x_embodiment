{
    "summary": "This code utilizes TensorFlow, TFDS, Jax/PyTorch for efficient robotics dataset loading and optimization. It aligns dataset specs, caches, and prevents data exhaustion while converting RLDS datasets to Open X Embodiment format for reinforcement learning tasks in RoboNet and MT_Opt.",
    "details": [
        {
            "comment": "The code imports necessary libraries and defines a list of dataset names for Open X Embodiment. These datasets contain various robotic tasks, conversions, and real-life scenarios converted externally to RLDs format.",
            "location": "\"/media/root/Prima/works/open_x_embodiment/docs/src/colabs/Open_X_Embodiment_Datasets.txt\":0-29",
            "content": "import numpy as np\nimport tensorflow_datasets as tfds\nfrom PIL import Image\nfrom IPython import display\nDATASETS = [\n    'fractal20220817_data',\n    'kuka',\n    'bridge',\n    'taco_play',\n    'jaco_play',\n    'berkeley_cable_routing',\n    'roboturk',\n    'nyu_door_opening_surprising_effectiveness',\n    'viola',\n    'berkeley_autolab_ur5',\n    'toto',\n    'language_table',\n    'columbia_cairlab_pusht_real',\n    'stanford_kuka_multimodal_dataset_converted_externally_to_rlds',\n    'nyu_rot_dataset_converted_externally_to_rlds',\n    'stanford_hydra_dataset_converted_externally_to_rlds',\n    'austin_buds_dataset_converted_externally_to_rlds',\n    'nyu_franka_play_dataset_converted_externally_to_rlds',\n    'maniskill_dataset_converted_externally_to_rlds',\n    'cmu_franka_exploration_dataset_converted_externally_to_rlds',\n    'ucsd_kitchen_dataset_converted_externally_to_rlds',\n    'ucsd_pick_and_place_dataset_converted_externally_to_rlds',\n    'austin_sailor_dataset_converted_externally_to_rlds',\n    'austin_sirius_dataset_converted_externally_to_rlds',"
        },
        {
            "comment": "This code is a list of dataset names related to robotics tasks converted externally to the RLDS (Robot Learning Dataset Standard) format. The list includes datasets from various universities and research institutions, covering different types of robotic actions and environments.",
            "location": "\"/media/root/Prima/works/open_x_embodiment/docs/src/colabs/Open_X_Embodiment_Datasets.txt\":30-50",
            "content": "    'bc_z',\n    'usc_cloth_sim_converted_externally_to_rlds',\n    'utokyo_pr2_opening_fridge_converted_externally_to_rlds',\n    'utokyo_pr2_tabletop_manipulation_converted_externally_to_rlds',\n    'utokyo_saytap_converted_externally_to_rlds',\n    'utokyo_xarm_pick_and_place_converted_externally_to_rlds',\n    'utokyo_xarm_bimanual_converted_externally_to_rlds',\n    'robo_net',\n    'berkeley_mvp_converted_externally_to_rlds',\n    'berkeley_rpt_converted_externally_to_rlds',\n    'kaist_nonprehensile_converted_externally_to_rlds',\n    'stanford_mask_vit_converted_externally_to_rlds',\n    'tokyo_u_lsmo_converted_externally_to_rlds',\n    'dlr_sara_pour_converted_externally_to_rlds',\n    'dlr_sara_grid_clamp_converted_externally_to_rlds',\n    'dlr_edan_shared_control_converted_externally_to_rlds',\n    'asu_table_top_converted_externally_to_rlds',\n    'stanford_robocook_converted_externally_to_rlds',\n    'eth_agent_affordances',\n    'imperialcollege_sawyer_wrist_cam',\n    'iamlab_cmu_pickup_insert_converted_externally_to_rlds',"
        },
        {
            "comment": "This code defines a function `dataset2path` that maps dataset names to their respective paths in cloud storage. It also includes functions for rendering images as a GIF and selecting a dataset path from a dropdown menu. The purpose is to display samples of multiple datasets by running the cell again after choosing the desired dataset in the dropdown.",
            "location": "\"/media/root/Prima/works/open_x_embodiment/docs/src/colabs/Open_X_Embodiment_Datasets.txt\":51-81",
            "content": "    'uiuc_d3field',\n    'utaustin_mutex',\n    'berkeley_fanuc_manipulation',\n    'cmu_play_fusion',\n    'cmu_stretch',\n    'berkeley_gnm_recon',\n    'berkeley_gnm_cory_hall',\n    'berkeley_gnm_sac_son'\n]\ndef dataset2path(dataset_name):\n  if dataset_name == 'robo_net':\n    version = '1.0.0'\n  elif dataset_name == 'language_table':\n    version = '0.0.1'\n  else:\n    version = '0.1.0'\n  return f'gs://gresearch/robotics/{dataset_name}/{version}'\ndef as_gif(images, path='temp.gif'):\n  # Render the images as the gif:\n  images[0].save(path, save_all=True, append_images=images[1:], duration=1000, loop=0)\n  gif_bytes = open(path,'rb').read()\n  return gif_bytes\n# choose the dataset path in the dropdown on the right and rerun this cell\n# to see multiple samples\ndataset = 'fractal20220817_data' # @param ['fractal20220817_data', 'kuka', 'bridge', 'taco_play', 'jaco_play', 'berkeley_cable_routing', 'roboturk', 'nyu_door_opening_surprising_effectiveness', 'viola', 'berkeley_autolab_ur5', 'toto', 'language_table', 'columbia_c"
        },
        {
            "comment": "The code is listing a series of external datasets that have been converted to the RLDs format, specifically for Open X Embodiment. These datasets represent various robotic manipulation tasks and interactions across different platforms and environments.",
            "location": "\"/media/root/Prima/works/open_x_embodiment/docs/src/colabs/Open_X_Embodiment_Datasets.txt\":81-81",
            "content": "airlab_pusht_real', 'stanford_kuka_multimodal_dataset_converted_externally_to_rlds', 'nyu_rot_dataset_converted_externally_to_rlds', 'stanford_hydra_dataset_converted_externally_to_rlds', 'austin_buds_dataset_converted_externally_to_rlds', 'nyu_franka_play_dataset_converted_externally_to_rlds', 'maniskill_dataset_converted_externally_to_rlds', 'furniture_bench_dataset_converted_externally_to_rlds', 'cmu_franka_exploration_dataset_converted_externally_to_rlds', 'ucsd_kitchen_dataset_converted_externally_to_rlds', 'ucsd_pick_and_place_dataset_converted_externally_to_rlds', 'austin_sailor_dataset_converted_externally_to_rlds', 'austin_sirius_dataset_converted_externally_to_rlds', 'bc_z', 'usc_cloth_sim_converted_externally_to_rlds', 'utokyo_pr2_opening_fridge_converted_externally_to_rlds', 'utokyo_pr2_tabletop_manipulation_converted_externally_to_rlds', 'utokyo_saytap_converted_externally_to_rlds', 'utokyo_xarm_pick_and_place_converted_externally_to_rlds', 'utokyo_xarm_bimanual_converted_"
        },
        {
            "comment": "The code is checking if the specified display key ('image') exists in the feature list of steps observation for a dataset. If it doesn't exist, it raises a ValueError. The code retrieves the builder from a directory using tfds (TensorFlow Datasets) and checks if the 'image' key is present in the dataset information features.",
            "location": "\"/media/root/Prima/works/open_x_embodiment/docs/src/colabs/Open_X_Embodiment_Datasets.txt\":81-86",
            "content": "externally_to_rlds', 'robo_net', 'berkeley_mvp_converted_externally_to_rlds', 'berkeley_rpt_converted_externally_to_rlds', 'kaist_nonprehensile_converted_externally_to_rlds', 'stanford_mask_vit_converted_externally_to_rlds', 'tokyo_u_lsmo_converted_externally_to_rlds', 'dlr_sara_pour_converted_externally_to_rlds', 'dlr_sara_grid_clamp_converted_externally_to_rlds', 'dlr_edan_shared_control_converted_externally_to_rlds', 'asu_table_top_converted_externally_to_rlds', 'stanford_robocook_converted_externally_to_rlds', 'eth_agent_affordances', 'imperialcollege_sawyer_wrist_cam', 'iamlab_cmu_pickup_insert_converted_externally_to_rlds', 'uiuc_d3field', 'utaustin_mutex', 'berkeley_fanuc_manipulation', 'cmu_food_manipulation', 'cmu_play_fusion', 'cmu_stretch', 'berkeley_gnm_recon', 'berkeley_gnm_cory_hall', 'berkeley_gnm_sac_son']\ndisplay_key = 'image'\nb = tfds.builder_from_directory(builder_dir=dataset2path(dataset))\nif display_key not in b.info.features['steps']['observation']:\n  raise ValueError("
        },
        {
            "comment": "This code is loading a dataset and displaying images for the first 10 episodes in train split. It also prints other elements of the episode steps, and suggests installing tfds-nightly for up-to-date datasets. The dataset names are hardcoded but can be replaced with a list from a Google sheet.",
            "location": "\"/media/root/Prima/works/open_x_embodiment/docs/src/colabs/Open_X_Embodiment_Datasets.txt\":87-108",
            "content": "      f\"The key {display_key} was not found in this dataset.\\n\"\n      + \"Please choose a different image key to display for this dataset.\\n\"\n      + \"Here is the observation spec:\\n\"\n      + str(b.info.features['steps']['observation']))\nds = b.as_dataset(split='train[:10]').shuffle(10)   # take only first 10 episodes\nepisode = next(iter(ds))\nimages = [step['observation'][display_key] for step in episode['steps']]\nimages = [Image.fromarray(image.numpy()) for image in images]\ndisplay.Image(as_gif(images))\n# other elements of the episode step --> this may vary for each dataset\nfor elem in next(iter(episode['steps'])).items():\n  print(elem)\n!pip install tfds-nightly   # to get most up-to-date registered datasets\nimport tensorflow_datasets as tfds\nimport tqdm\n# optionally replace the DATASET_NAMES below with the list of filtered datasets from the google sheet\nDATASET_NAMES = ['fractal_20220817_data', 'kuka', 'bridge', 'taco_play', 'jaco_play', 'berkeley_cable_routing', 'roboturk', 'nyu_door_opening_surpris"
        },
        {
            "comment": "The code contains a list of dataset names, most of which have been converted externally to RLDs format. These datasets are likely used for robot learning and dexterity tasks across various platforms and research institutions.",
            "location": "\"/media/root/Prima/works/open_x_embodiment/docs/src/colabs/Open_X_Embodiment_Datasets.txt\":108-108",
            "content": "ing_effectiveness', 'viola', 'berkeley_autolab_ur5', 'toto', 'language_table', 'columbia_cairlab_pusht_real', 'stanford_kuka_multimodal_dataset_converted_externally_to_rlds', 'nyu_rot_dataset_converted_externally_to_rlds', 'stanford_hydra_dataset_converted_externally_to_rlds', 'austin_buds_dataset_converted_externally_to_rlds', 'nyu_franka_play_dataset_converted_externally_to_rlds', 'maniskill_dataset_converted_externally_to_rlds', 'furniture_bench_dataset_converted_externally_to_rlds', 'cmu_franka_exploration_dataset_converted_externally_to_rlds', 'ucsd_kitchen_dataset_converted_externally_to_rlds', 'ucsd_pick_and_place_dataset_converted_externally_to_rlds', 'austin_sailor_dataset_converted_externally_to_rlds', 'austin_sirius_dataset_converted_externally_to_rlds', 'bc_z', 'usc_cloth_sim_converted_externally_to_rlds', 'utokyo_pr2_opening_fridge_converted_externally_to_rlds', 'utokyo_pr2_tabletop_manipulation_converted_externally_to_rlds', 'utokyo_saytap_converted_externally_to_rlds', '"
        },
        {
            "comment": "Code is downloading a list of datasets to a specified directory. The dataset names include \"utokyo_xarm_pick_and_place\" and \"berkeley_rpt\". The download directory is set as \"~/tensorflow_datasets\". The code is printing the number of datasets being downloaded.",
            "location": "\"/media/root/Prima/works/open_x_embodiment/docs/src/colabs/Open_X_Embodiment_Datasets.txt\":108-111",
            "content": "utokyo_xarm_pick_and_place_converted_externally_to_rlds', 'utokyo_xarm_bimanual_converted_externally_to_rlds', 'robo_net', 'berkeley_mvp_converted_externally_to_rlds', 'berkeley_rpt_converted_externally_to_rlds', 'kaist_nonprehensile_converted_externally_to_rlds', 'stanford_mask_vit_converted_externally_to_rlds', 'tokyo_u_lsmo_converted_externally_to_rlds', 'dlr_sara_pour_converted_externally_to_rlds', 'dlr_sara_grid_clamp_converted_externally_to_rlds', 'dlr_edan_shared_control_converted_externally_to_rlds', 'asu_table_top_converted_externally_to_rlds', 'stanford_robocook_converted_externally_to_rlds', 'eth_agent_affordances', 'imperialcollege_sawyer_wrist_cam', 'iamlab_cmu_pickup_insert_converted_externally_to_rlds', 'uiuc_d3field', 'utaustin_mutex', 'berkeley_fanuc_manipulation', 'cmu_food_manipulation', 'cmu_play_fusion', 'cmu_stretch', 'berkeley_gnm_recon', 'berkeley_gnm_cory_hall', 'berkeley_gnm_sac_son']\nDOWNLOAD_DIR = '~/tensorflow_datasets'\nprint(f\"Downloading {len(DATASET_NAMES)} datasets to {DOWNLOAD_DIR}.\")"
        },
        {
            "comment": "The code loads a specified dataset (e.g., 'kuka'), converts the RLDS episode dataset into individual steps, reformats the data, shuffles, repeats, pre-fetches, and caches the dataset for efficient processing. It utilizes TensorFlow and TensorFlow Datasets libraries.",
            "location": "\"/media/root/Prima/works/open_x_embodiment/docs/src/colabs/Open_X_Embodiment_Datasets.txt\":112-146",
            "content": "for dataset_name in tqdm.tqdm(DATASET_NAMES):\n  _ = tfds.load(dataset_name, data_dir=DOWNLOAD_DIR)\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n# load raw dataset --> replace this with tfds.load(<dataset_name>) on your\n# local machine!\ndataset = 'kuka'\nb = tfds.builder_from_directory(builder_dir=dataset2path(dataset))\nds = b.as_dataset(split='train[:10]')\ndef episode2steps(episode):\n  return episode['steps']\ndef step_map_fn(step):\n  return {\n      'observation': {\n          'image': tf.image.resize(step['observation']['image'], (128, 128)),\n      },\n      'action': tf.concat([\n          step['action']['world_vector'],\n          step['action']['rotation_delta'],\n          step['action']['gripper_closedness_action'],\n      ], axis=-1)\n  }\n# convert RLDS episode dataset to individual steps & reformat\nds = ds.map(\n    episode2steps, num_parallel_calls=tf.data.AUTOTUNE).flat_map(lambda x: x)\nds = ds.map(step_map_fn, num_parallel_calls=tf.data.AUTOTUNE)\n# shuffle, repeat, pre-fetch, batch\nds = ds.cache()         # optionally keep full dataset in memory"
        },
        {
            "comment": "The code shuffles and repeats the dataset to ensure data never runs out, uses tqdm for progress tracking, and trains using Jax/PyTorch on each batch until i == 10000. It then loads a second dataset (tfds.load(<dataset_name>)), aligns its specs with the first dataset using step_map_fn_mutex, shuffles and repeats this dataset as well. Finally, it caches, shuffles, and repeats the combined datasets.",
            "location": "\"/media/root/Prima/works/open_x_embodiment/docs/src/colabs/Open_X_Embodiment_Datasets.txt\":147-178",
            "content": "ds = ds.shuffle(100)    # set shuffle buffer size\nds = ds.repeat()        # ensure that data never runs out\nimport tqdm\nfor i, batch in tqdm.tqdm(\n    enumerate(ds.prefetch(3).batch(4).as_numpy_iterator())):\n  # here you would add your Jax / PyTorch training code\n  if i == 10000: break\n# Load second dataset --> replace this with tfds.load(<dataset_name>) on your\n# local machine!\ndataset = 'utaustin_mutex'\nb = tfds.builder_from_directory(builder_dir=dataset2path(dataset))\nds2 = b.as_dataset(split='train[:10]')\ndef step_map_fn_mutex(step):\n  # reformat to align specs of both datasets\n  return {\n      'observation': {\n          'image': tf.image.resize(step['observation']['image'], (128, 128)),\n      },\n      'action': step['action'],\n  }\nds2 = ds2.map(\n    episode2steps, num_parallel_calls=tf.data.AUTOTUNE).flat_map(lambda x: x)\nds2 = ds2.map(step_map_fn_mutex, num_parallel_calls=tf.data.AUTOTUNE)\n# shuffle, repeat, pre-fetch, batch\nds2 = ds2.cache()         # optionally keep full dataset in memory\nds2 = ds2.shuffle(100)    # set shuffle buffer size"
        },
        {
            "comment": "The code is preparing and loading datasets for training in TensorFlow. It ensures data never runs out by repeating the dataset, interleaves datasets with equal sampling weight, and then iterates over the combined dataset using TensorFlow's iterator. The code also installs the \"rl-datasets\" package and imports necessary libraries for working with TensorFlow datasets and Reverb, a distributed replay buffer for RL algorithms. It defines functions to convert TFDS features into TensorSpec, which is used in defining data pipelines.",
            "location": "\"/media/root/Prima/works/open_x_embodiment/docs/src/colabs/Open_X_Embodiment_Datasets.txt\":179-220",
            "content": "ds2 = ds2.repeat()        # ensure that data never runs out\n# interleave datasets w/ equal sampling weight\nds_combined = tf.data.Dataset.sample_from_datasets([ds, ds2], [0.5, 0.5])\nimport tqdm\nfor i, batch in tqdm.tqdm(\n    enumerate(ds_combined.prefetch(3).batch(4).as_numpy_iterator())):\n  # here you would add your Jax / PyTorch training code\n  if i == 10000: break\n!pip install rlds[tensorflow]\nfrom typing import Any, Dict, Union, NamedTuple\nimport numpy as np\nimport tensorflow_datasets as tfds\nimport rlds\nimport reverb\nfrom rlds import transformations\nimport tensorflow_datasets as tfds\nimport tree\nimport abc\nimport dataclasses\nfrom typing import Dict, Optional\nfrom rlds import rlds_types\nimport tensorflow as tf\nfrom PIL import Image\nfrom IPython import display\n# @title Transformation definitions\ndef _features_to_tensor_spec(\n    feature: tfds.features.FeatureConnector\n) -> tf.TensorSpec:\n  \"\"\"Converts a tfds Feature into a TensorSpec.\"\"\"\n  def _get_feature_spec(nested_feature: tfds.features.FeatureConnector):\n    if isinstance(nested_feature, tf.DType):"
        },
        {
            "comment": "This code defines a function _get_feature_spec that returns the tensor spec for nested features, and another function _encoded_feature that applies encoding to Images and/or Tensors if specified. The tfds library is used for defining images with shape, dtype, use_colormap, and encoding format.",
            "location": "\"/media/root/Prima/works/open_x_embodiment/docs/src/colabs/Open_X_Embodiment_Datasets.txt\":221-243",
            "content": "      return tf.TensorSpec(shape=(), dtype=nested_feature)\n    else:\n      return nested_feature.get_tensor_spec()\n  # FeaturesDict can sometimes be a plain dictionary, so we use tf.nest to\n  # make sure we deal with the nested structure.\n  return tf.nest.map_structure(_get_feature_spec, feature)\ndef _encoded_feature(feature: Optional[tfds.features.FeatureConnector],\n                     image_encoding: Optional[str],\n                     tensor_encoding: Optional[tfds.features.Encoding]):\n  \"\"\"Adds encoding to Images and/or Tensors.\"\"\"\n  def _apply_encoding(feature: tfds.features.FeatureConnector,\n                      image_encoding: Optional[str],\n                      tensor_encoding: Optional[tfds.features.Encoding]):\n    if image_encoding and isinstance(feature, tfds.features.Image):\n      return tfds.features.Image(\n          shape=feature.shape,\n          dtype=feature.dtype,\n          use_colormap=feature.use_colormap,\n          encoding_format=image_encoding)\n    if tensor_encoding and isinstance("
        },
        {
            "comment": "The code snippet defines a class RLDSSpec which is a specification for an RLDS (Reinforcement Learning Dataset) and includes observation, action, reward, discount, step_metadata, and episode_metadata information. The method step_tensor_spec() returns the TensorSpec of an RLDS step. The code also has a helper function _apply_encoding() for encoding features.",
            "location": "\"/media/root/Prima/works/open_x_embodiment/docs/src/colabs/Open_X_Embodiment_Datasets.txt\":244-270",
            "content": "        feature, tfds.features.Tensor) and feature.dtype != tf.string:\n      return tfds.features.Tensor(\n          shape=feature.shape, dtype=feature.dtype, encoding=tensor_encoding)\n    return feature\n  if not feature:\n    return None\n  return tf.nest.map_structure(\n      lambda x: _apply_encoding(x, image_encoding, tensor_encoding), feature)\n@dataclasses.dataclass\nclass RLDSSpec(metaclass=abc.ABCMeta):\n  \"\"\"Specification of an RLDS Dataset.\n  It is used to hold a spec that can be converted into a TFDS DatasetInfo or\n  a `tf.data.Dataset` spec.\n  \"\"\"\n  observation_info: Optional[tfds.features.FeatureConnector] = None\n  action_info: Optional[tfds.features.FeatureConnector] = None\n  reward_info: Optional[tfds.features.FeatureConnector] = None\n  discount_info: Optional[tfds.features.FeatureConnector] = None\n  step_metadata_info: Optional[tfds.features.FeaturesDict] = None\n  episode_metadata_info: Optional[tfds.features.FeaturesDict] = None\n  def step_tensor_spec(self) -> Dict[str, tf.TensorSpec]:\n    \"\"\"Obtains the TensorSpec of an RLDS step.\"\"\""
        },
        {
            "comment": "This code defines the TensorSpec for an RLDS step and episode. It creates a dictionary of TensorSpecs for observation, action, discount, reward, and step metadata. It also adds boolean TensorSpecs for first, last, and terminal status. The episode_tensor_spec function returns the TensorSpec dictionary.",
            "location": "\"/media/root/Prima/works/open_x_embodiment/docs/src/colabs/Open_X_Embodiment_Datasets.txt\":271-297",
            "content": "    step = {}\n    if self.observation_info:\n      step[rlds_types.OBSERVATION] = _features_to_tensor_spec(\n          self.observation_info)\n    if self.action_info:\n      step[rlds_types.ACTION] = _features_to_tensor_spec(\n          self.action_info)\n    if self.discount_info:\n      step[rlds_types.DISCOUNT] = _features_to_tensor_spec(\n          self.discount_info)\n    if self.reward_info:\n      step[rlds_types.REWARD] = _features_to_tensor_spec(\n          self.reward_info)\n    if self.step_metadata_info:\n      for k, v in self.step_metadata_info.items():\n        step[k] = _features_to_tensor_spec(v)\n    step[rlds_types.IS_FIRST] = tf.TensorSpec(shape=(), dtype=bool)\n    step[rlds_types.IS_LAST] = tf.TensorSpec(shape=(), dtype=bool)\n    step[rlds_types.IS_TERMINAL] = tf.TensorSpec(shape=(), dtype=bool)\n    return step\n  def episode_tensor_spec(self) -> Dict[str, tf.TensorSpec]:\n    \"\"\"Obtains the TensorSpec of an RLDS step.\"\"\"\n    episode = {}\n    episode[rlds_types.STEPS] = tf.data.DatasetSpec(\n        element_spec=self.step_tensor_spec())"
        },
        {
            "comment": "This code appears to be part of a class, possibly for dataset configuration. It includes methods to convert features into tensors and create a DatasetConfig object using information from the class instance. The 'to_dataset_config' method takes optional parameters for image and tensor encodings, as well as metadata like citation, homepage, and overall description.",
            "location": "\"/media/root/Prima/works/open_x_embodiment/docs/src/colabs/Open_X_Embodiment_Datasets.txt\":298-323",
            "content": "    if self.episode_metadata_info:\n      for k, v in self.episode_metadata_info.items():\n        episode[k] = _features_to_tensor_spec(v)\n    return episode\n  def to_dataset_config(\n      self,\n      name: str,\n      image_encoding: Optional[str] = None,\n      tensor_encoding: Optional[tfds.features.Encoding] = None,\n      citation: Optional[str] = None,\n      homepage: Optional[str] = None,\n      description: Optional[str] = None,\n      overall_description: Optional[str] = None,\n  ) -> tfds.rlds.rlds_base.DatasetConfig:\n    \"\"\"Obtains the DatasetConfig for TFDS from the Spec.\"\"\"\n    return tfds.rlds.rlds_base.DatasetConfig(\n        name=name,\n        description=description,\n        overall_description=overall_description,\n        homepage=homepage,\n        citation=citation,\n        observation_info=_encoded_feature(self.observation_info, image_encoding,\n                                          tensor_encoding),\n        action_info=_encoded_feature(self.action_info, image_encoding,\n                                     tensor_encoding),"
        },
        {
            "comment": "The code defines a function \"to_features_dict()\" that returns a TensorFlow Dataset (TFDS) FeaturesDict representing the dataset configuration. It creates a step config dictionary with keys for IS_FIRST, IS_LAST, and IS_TERMINAL properties, and adds observation, action, and discount information if available. The _encoded_feature function is used to encode reward_info, discount_info, step_metadata_info, and episode_metadata_info.",
            "location": "\"/media/root/Prima/works/open_x_embodiment/docs/src/colabs/Open_X_Embodiment_Datasets.txt\":324-346",
            "content": "        reward_info=_encoded_feature(self.reward_info, image_encoding,\n                                     tensor_encoding),\n        discount_info=_encoded_feature(self.discount_info, image_encoding,\n                                       tensor_encoding),\n        step_metadata_info=_encoded_feature(self.step_metadata_info,\n                                            image_encoding, tensor_encoding),\n        episode_metadata_info=_encoded_feature(self.episode_metadata_info,\n                                               image_encoding, tensor_encoding))\n  def to_features_dict(self):\n    \"\"\"Returns a TFDS FeaturesDict representing the dataset config.\"\"\"\n    step_config = {\n        rlds_types.IS_FIRST: tf.bool,\n        rlds_types.IS_LAST: tf.bool,\n        rlds_types.IS_TERMINAL: tf.bool,\n    }\n    if self.observation_info:\n      step_config[rlds_types.OBSERVATION] = self.observation_info\n    if self.action_info:\n      step_config[rlds_types.ACTION] = self.action_info\n    if self.discount_info:\n      step_config[rlds_types.DISCOUNT] = self.discount_info"
        },
        {
            "comment": "This code defines a class for transforming RLDS episodes into trajectories, involving three stages. It accepts episode-level and step-level information as input. The `episode_to_steps_map_fn` can be used to select or modify steps. This is part of the Open X Embodiment dataset structure.",
            "location": "\"/media/root/Prima/works/open_x_embodiment/docs/src/colabs/Open_X_Embodiment_Datasets.txt\":347-377",
            "content": "    if self.reward_info:\n      step_config[rlds_types.REWARD] = self.reward_info\n    if self.step_metadata_info:\n      for k, v in self.step_metadata_info.items():\n        step_config[k] = v\n    if self.episode_metadata_info:\n      return tfds.features.FeaturesDict({\n          rlds_types.STEPS: tfds.features.Dataset(step_config),\n          **self.episode_metadata_info,\n      })\n    else:\n      return tfds.features.FeaturesDict({\n          rlds_types.STEPS: tfds.features.Dataset(step_config),\n      })\nRLDS_SPEC = RLDSSpec\nTENSOR_SPEC = Union[tf.TensorSpec, dict[str, tf.TensorSpec]]\n@dataclasses.dataclass\nclass TrajectoryTransform(metaclass=abc.ABCMeta):\n  \"\"\"Specification the TrajectoryTransform applied to a dataset of episodes.\n  A TrajectoryTransform is a set of rules transforming a dataset\n  of RLDS episodes to a dataset of trajectories.\n  This involves three distinct stages:\n  - An optional `episode_to_steps_map_fn(episode)` is called at the episode\n    level, and can be used to select or modify steps.\n    - Augmentation: an `episode_key` could be propagated to `steps` for"
        },
        {
            "comment": "This code defines a TrajectoryTransform class that takes in an episode dataset and transforms it into flattened steps. It can select specific steps, strip features from steps, use step_map_fn for feature manipulation, utilize pattern to slice episodes to overlapping trajectories, and define expected_tensor_spec for the resulting dataset.",
            "location": "\"/media/root/Prima/works/open_x_embodiment/docs/src/colabs/Open_X_Embodiment_Datasets.txt\":378-401",
            "content": "      debugging.\n    - Selection: Particular steps can be selected.\n    - Stripping: Features can be removed from steps. Prefer using `step_map_fn`.\n  - An optional `step_map_fn` is called at the flattened steps dataset for each\n    step, and can be used to featurize a step, e.g. add/remove features, or\n    augument images\n  - A `pattern` leverages DM patterns to set a rule of slicing an episode to a\n    dataset of overlapping trajectories.\n  Importantly, each TrajectoryTransform must define a `expected_tensor_spec`\n  which specifies a nested TensorSpec of the resulting dataset. This is what\n  this TrajectoryTransform will produce, and can be used as an interface with\n  a neural network.\n  \"\"\"\n  episode_dataset_spec: RLDS_SPEC\n  episode_to_steps_fn_dataset_spec: RLDS_SPEC\n  steps_dataset_spec: Any\n  pattern: reverb.structured_writer.Pattern\n  episode_to_steps_map_fn: Any\n  expected_tensor_spec: TENSOR_SPEC\n  step_map_fn: Optional[Any] = None\n  def get_for_cached_trajectory_transform(self):\n    \"\"\"Creates a copy of this traj transform to use with caching."
        },
        {
            "comment": "This code defines a `TrajectoryTransform` class that can be used to transform an episodic RLDS dataset into a steps-based RLDS dataset. The class has methods for initializing the transform and applying it to datasets of episodes or steps. When creating a copy of the transform, it uses a default version of the `episode_to_steps_map_fn` if the effect of that function has already been materialized in the cached copy of the dataset. The `transform_episodic_rlds_dataset` method applies the transform to the dataset of episodes by converting it into a dataset of steps and then creating a new pattern dataset. Another method, `transform_steps_rlds_dataset`, is not shown but likely performs similar transformation operations on steps-based RLDS datasets.",
            "location": "\"/media/root/Prima/works/open_x_embodiment/docs/src/colabs/Open_X_Embodiment_Datasets.txt\":403-425",
            "content": "    The returned TrajectoryTransfrom copy will be initialized with the default\n    version of the `episode_to_steps_map_fn`, because the effect of that\n    function has already been materialized in the cached copy of the dataset.\n    Returns:\n      trajectory_transform: A copy of the TrajectoryTransform with overridden\n        `episode_to_steps_map_fn`.\n    \"\"\"\n    traj_copy = dataclasses.replace(self)\n    traj_copy.episode_dataset_spec = traj_copy.episode_to_steps_fn_dataset_spec\n    traj_copy.episode_to_steps_map_fn = lambda e: e[rlds_types.STEPS]\n    return traj_copy\n  def transform_episodic_rlds_dataset(self, episodes_dataset: tf.data.Dataset):\n    \"\"\"Applies this TrajectoryTransform to the dataset of episodes.\"\"\"\n    # Convert the dataset of episodes to the dataset of steps.\n    steps_dataset = episodes_dataset.map(\n        self.episode_to_steps_map_fn, num_parallel_calls=tf.data.AUTOTUNE\n    ).flat_map(lambda x: x)\n    return self._create_pattern_dataset(steps_dataset)\n  def transform_steps_rlds_dataset("
        },
        {
            "comment": "This code defines a class with methods for creating datasets of trajectories. The `__call__` method applies the TrajectoryTransform to the dataset of episode steps. The `create_test_dataset` function generates a test dataset of zeros that has the same structure as the target dataset. Finally, the `_create_pattern_dataset` function creates a PatternDataset from the `steps_dataset`, possibly transforming each step if a `step_map_fn` is provided.",
            "location": "\"/media/root/Prima/works/open_x_embodiment/docs/src/colabs/Open_X_Embodiment_Datasets.txt\":426-453",
            "content": "      self, steps_dataset: tf.data.Dataset\n  ) -> tf.data.Dataset:\n    \"\"\"Applies this TrajectoryTransform to the dataset of episode steps.\"\"\"\n    return self._create_pattern_dataset(steps_dataset)\n  def create_test_dataset(\n      self,\n  ) -> tf.data.Dataset:\n    \"\"\"Creates a test dataset of trajectories.\n    It is guaranteed that the structure of this dataset will be the same as\n    when flowing real data. Hence this is a useful construct for tests or\n    initialization of JAX models.\n    Returns:\n      dataset: A test dataset made of zeros structurally identical to the\n        target dataset of trajectories.\n    \"\"\"\n    zeros = transformations.zeros_from_spec(self.expected_tensor_spec)\n    return tf.data.Dataset.from_tensors(zeros)\n  def _create_pattern_dataset(\n      self, steps_dataset: tf.data.Dataset) -> tf.data.Dataset:\n    \"\"\"Create PatternDataset from the `steps_dataset`.\"\"\"\n    config = create_structured_writer_config('temp', self.pattern)\n    # Further transform each step if the `step_map_fn` is provided."
        },
        {
            "comment": "This code defines a `TrajectoryTransformBuilder` class that facilitates the creation of a `TrajectoryTransform`. The class has an initializer with parameters for dataset spec, step map function, pattern function, and expected tensor spec. The `build()` method creates a `TrajectoryTransform` from the builder instance. A valid expected tensor spec can be optionally validated.",
            "location": "\"/media/root/Prima/works/open_x_embodiment/docs/src/colabs/Open_X_Embodiment_Datasets.txt\":454-482",
            "content": "    if self.step_map_fn:\n      steps_dataset = steps_dataset.map(self.step_map_fn)\n    pattern_dataset = reverb.PatternDataset(\n        input_dataset=steps_dataset,\n        configs=[config],\n        respect_episode_boundaries=True,\n        is_end_of_episode=lambda x: x[rlds_types.IS_LAST])\n    return pattern_dataset\nclass TrajectoryTransformBuilder(object):\n  \"\"\"Facilitates creation of the `TrajectoryTransform`.\"\"\"\n  def __init__(self,\n               dataset_spec: RLDS_SPEC,\n               episode_to_steps_map_fn=lambda e: e[rlds_types.STEPS],\n               step_map_fn=None,\n               pattern_fn=None,\n               expected_tensor_spec=None):\n    self._rds_dataset_spec = dataset_spec\n    self._steps_spec = None\n    self._episode_to_steps_map_fn = episode_to_steps_map_fn\n    self._step_map_fn = step_map_fn\n    self._pattern_fn = pattern_fn\n    self._expected_tensor_spec = expected_tensor_spec\n  def build(self,\n            validate_expected_tensor_spec: bool = True) -> TrajectoryTransform:\n    \"\"\"Creates `TrajectoryTransform` from a `TrajectoryTransformBuilder`.\"\"\""
        },
        {
            "comment": "The code is validating the expected tensor spec and creating a dataset for zero episodes. It applies step transformation if needed, creates a reference step, defines a pattern, and creates a target tensor structure for reverb table signature. If the tensor spec of the TrajectoryTransform doesn't match the expected spec, it raises an error.",
            "location": "\"/media/root/Prima/works/open_x_embodiment/docs/src/colabs/Open_X_Embodiment_Datasets.txt\":484-511",
            "content": "    if validate_expected_tensor_spec and self._expected_tensor_spec is None:\n      raise ValueError('`expected_tensor_spec` must be set.')\n    episode_ds = zero_episode_dataset_from_spec(self._rds_dataset_spec)\n    steps_ds = episode_ds.flat_map(self._episode_to_steps_map_fn)\n    episode_to_steps_fn_dataset_spec = self._rds_dataset_spec\n    if self._step_map_fn is not None:\n      steps_ds = steps_ds.map(self._step_map_fn)\n    zeros_spec = transformations.zeros_from_spec(steps_ds.element_spec)  # pytype: disable=wrong-arg-types\n    ref_step = reverb.structured_writer.create_reference_step(zeros_spec)\n    pattern = self._pattern_fn(ref_step)\n    steps_ds_spec = steps_ds.element_spec\n    target_tensor_structure = create_reverb_table_signature(\n        'temp_table', steps_ds_spec, pattern)\n    if (validate_expected_tensor_spec and\n        self._expected_tensor_spec != target_tensor_structure):\n      raise RuntimeError(\n          'The tensor spec of the TrajectoryTransform doesn\\'t '\n          'match the expected spec.\\n'"
        },
        {
            "comment": "This function creates a zero-valued dataset of episodes from an RLDS spec. It uses the add_steps() function to fill episodes with zeros and removes any 'fake' keys if present. The returned TrajectoryTransform represents this new dataset.\n\nQuestion:",
            "location": "\"/media/root/Prima/works/open_x_embodiment/docs/src/colabs/Open_X_Embodiment_Datasets.txt\":512-536",
            "content": "          'Expected:\\n%s\\nActual:\\n%s\\n' %\n          (str(self._expected_tensor_spec).replace('TensorSpec',\n                                                   'tf.TensorSpec'),\n           str(target_tensor_structure).replace('TensorSpec', 'tf.TensorSpec')))\n    return TrajectoryTransform(\n        episode_dataset_spec=self._rds_dataset_spec,\n        episode_to_steps_fn_dataset_spec=episode_to_steps_fn_dataset_spec,\n        steps_dataset_spec=steps_ds_spec,\n        pattern=pattern,\n        episode_to_steps_map_fn=self._episode_to_steps_map_fn,\n        step_map_fn=self._step_map_fn,\n        expected_tensor_spec=target_tensor_structure)\ndef zero_episode_dataset_from_spec(rlds_spec: RLDS_SPEC):\n  \"\"\"Creates a zero valued dataset of episodes for the given RLDS Spec.\"\"\"\n  def add_steps(episode, step_spec):\n    episode[rlds_types.STEPS] = transformations.zero_dataset_like(\n        tf.data.DatasetSpec(step_spec))\n    if 'fake' in episode:\n      del episode['fake']\n    return episode\n  episode_without_steps_spec = {"
        },
        {
            "comment": "This code is creating an episodes dataset with or without steps and then returning it. It also creates a reverb table specification using structured writer configuration and pattern for storing data in Reverb. The code seems to be part of a larger program that involves working with Reverb, likely for storing and processing data related to episodes.",
            "location": "\"/media/root/Prima/works/open_x_embodiment/docs/src/colabs/Open_X_Embodiment_Datasets.txt\":537-563",
            "content": "      k: v\n      for k, v in rlds_spec.episode_tensor_spec().items()\n      if k != rlds_types.STEPS\n  }\n  if episode_without_steps_spec:\n    episodes_dataset = transformations.zero_dataset_like(\n        tf.data.DatasetSpec(episode_without_steps_spec))\n  else:\n    episodes_dataset = tf.data.Dataset.from_tensors({'fake': ''})\n  episodes_dataset_with_steps = episodes_dataset.map(\n      lambda episode: add_steps(episode, rlds_spec.step_tensor_spec()))\n  return episodes_dataset_with_steps\ndef create_reverb_table_signature(table_name: str, steps_dataset_spec,\n                                  pattern: reverb.structured_writer.Pattern) -> reverb.reverb_types.SpecNest:\n  config = create_structured_writer_config(table_name, pattern)\n  reverb_table_spec = reverb.structured_writer.infer_signature(\n      [config], steps_dataset_spec)\n  return reverb_table_spec\ndef create_structured_writer_config(table_name: str,\n                                    pattern: reverb.structured_writer.Pattern) -> Any:\n  config = reverb.structured_writer.create_config("
        },
        {
            "comment": "The code imports necessary libraries and builds a dataset from the 'mt_opt' directory, creates a spec for the RLDSSpec, and uses the n_step_pattern_builder function to create trajectories of length 3.",
            "location": "\"/media/root/Prima/works/open_x_embodiment/docs/src/colabs/Open_X_Embodiment_Datasets.txt\":564-599",
            "content": "      pattern=pattern, table=table_name, conditions=[])\n  return config\ndef n_step_pattern_builder(n: int) -> Any:\n  \"\"\"Creates trajectory of length `n` from all fields of a `ref_step`.\"\"\"\n  def transform_fn(ref_step):\n    traj = {}\n    for key in ref_step:\n      if isinstance(ref_step[key], dict):\n        transformed_entry = tree.map_structure(lambda ref_node: ref_node[-n:],\n                                               ref_step[key])\n        traj[key] = transformed_entry\n      else:\n        traj[key] = ref_step[key][-n:]\n    return traj\n  return transform_fn\nimport tensorflow_datasets as tfds\nmt_opt_builder = tfds.builder_from_directory(builder_dir='gs://gresearch/robotics/mt_opt_rlds/1.0.0/')\nmt_opt_episodic_dataset = mt_opt_builder.as_dataset(split='train[:10]')\n# The RLDSSpec for the `mt_opt` dataset.\nmt_opt_rlds_spec = RLDSSpec(\n    observation_info=b.info.features['steps']['observation'],\n    action_info=b.info.features['steps']['action'],\n)\n# The following will create a trajectories of length 3.\ntrajectory_length = 3"
        },
        {
            "comment": "This code builds a trajectory transform, applies it to an episodic RLDS dataset, and then collects images from the first episode's transformed trajectory. It also creates a robo_net_builder for the episodic dataset and generates a GIF using collected images.",
            "location": "\"/media/root/Prima/works/open_x_embodiment/docs/src/colabs/Open_X_Embodiment_Datasets.txt\":600-625",
            "content": "trajectory_transform = TrajectoryTransformBuilder(mt_opt_rlds_spec, pattern_fn=n_step_pattern_builder(trajectory_length)).build(validate_expected_tensor_spec=False)\ntrajectory_dataset = trajectory_transform.transform_episodic_rlds_dataset(mt_opt_episodic_dataset)\ntrajectory_iter = iter(trajectory_dataset)\ntrajectory = next(trajectory_iter)\ntrajectory\n# Note that the leading dimension (3) corresponds to the trajectory_length\ntrajectory['observation']['image'].shape\nepisode = episodes[0]\n# Iterate over steps of the episode. Collect images.\nimages = [trajectory['observation']['image'][id] for id in range(trajectory['observation']['image'].shape[0])]\nimages = [Image.fromarray(image.numpy()) for image in images]\ndisplay.Image(as_gif(images))\nimport tensorflow_datasets as tfds\nrobo_net_builder = tfds.builder_from_directory(builder_dir='gs://gresearch/robotics/robo_net/1.0.0/')\nrobo_net_builder_episodic_dataset = robo_net_builder.as_dataset(split='train[:10]')\nepisodes = list(iter(robo_net_builder_episodic_dataset))"
        },
        {
            "comment": "The code defines a RLDS spec, step mapping functions for RoboNet and MT_Opt, and builds a trajectory transform using these specifications. It will create trajectories of length 3 to be used in reinforcement learning tasks. The transform builder includes a pattern function that is likely responsible for organizing the trajectories in a specific way.",
            "location": "\"/media/root/Prima/works/open_x_embodiment/docs/src/colabs/Open_X_Embodiment_Datasets.txt\":627-650",
            "content": "# The following will create a trajectories of length 3.\ntrajectory_length = 3\nrobo_net_rlds_spec = RLDSSpec(\n    observation_info=robo_net_builder.info.features['steps']['observation'],\n    action_info=robo_net_builder.info.features['steps']['action'],\n)\ndef robo_net_step_map_fn(step):\n  transformed_step = {}\n  transformed_step['observation'] = step['observation']['image']\n  transformed_step['is_first'] = step['is_first']\n  transformed_step['is_last'] = step['is_last']\n  transformed_step['is_terminal'] = step['is_terminal']\n  return transformed_step\nrobo_net_trajectory_transform = TrajectoryTransformBuilder(robo_net_rlds_spec,\n                                                                          step_map_fn=robo_net_step_map_fn,\n                                                                          pattern_fn=n_step_pattern_builder(trajectory_length)).build(validate_expected_tensor_spec=False)\ndef mt_opt_step_map_fn(step):\n  transformed_step = {}\n  transformed_step['observation'] = tf.cast(tf.ima"
        },
        {
            "comment": "The code resizes the observation image to be compatible with robo_net trajectory, then assigns is_first, is_last, and is_terminal values from the original step. It builds a TrajectoryTransformBuilder for mt_opt_rlds_spec, using a specific step map function and pattern builder. The code validates that the specs of robo_net_trajectory_transform and mt_opt_trajectory_transform are equal, then transforms episodic rlds datasets to create two trajectory datasets for normalized representations: robo_net_trajectory_dataset and mt_opt_trajectory_dataset.",
            "location": "\"/media/root/Prima/works/open_x_embodiment/docs/src/colabs/Open_X_Embodiment_Datasets.txt\":650-665",
            "content": "ge.resize(step['observation']['image'], [240, 320]), tf.uint8)  # Resize to be compatible with robo_net trajectory\n  transformed_step['is_first'] = step['is_first']\n  transformed_step['is_last'] = step['is_last']\n  transformed_step['is_terminal'] = step['is_terminal']\n  return transformed_step\nmt_opt_trajectory_transform = TrajectoryTransformBuilder(mt_opt_rlds_spec,\n                                                         step_map_fn=mt_opt_step_map_fn,\n                                                         pattern_fn=n_step_pattern_builder(trajectory_length)).build(validate_expected_tensor_spec=False)\n# Validate that the specs are equal\nassert robo_net_trajectory_transform.expected_tensor_spec == mt_opt_trajectory_transform.expected_tensor_spec\n# Create trajectory datasets for the two normalized representations:\nrobo_net_trajectory_dataset = robo_net_trajectory_transform.transform_episodic_rlds_dataset(robo_net_builder_episodic_dataset)\nmt_opt_trajectory_dataset = mt_opt_trajectory_transform.transform_episodic_rlds_dataset(mt_opt_episodic_dataset)"
        },
        {
            "comment": "The code samples from datasets and creates a combined dataset. It checks the size of each individual dataset, and ensures that TensorFlow Datasets is up-to-date for compatibility.",
            "location": "\"/media/root/Prima/works/open_x_embodiment/docs/src/colabs/Open_X_Embodiment_Datasets.txt\":668-694",
            "content": "combined_dataset = tf.data.Dataset.sample_from_datasets([robo_net_trajectory_dataset, mt_opt_trajectory_dataset])\ncombined_dataset = combined_dataset.batch(2)\ncombined_dataset_it = iter(combined_dataset)\nexample = next(combined_dataset_it)\n# First element of the batch returns a robot_net trajectory\nImage.fromarray(example['observation'].numpy()[0][0])\n# Second element of the batch returns a mt_opt trajectory\nImage.fromarray(example['observation'].numpy()[1][0])\n# Iterate over and make sure that a dataset can be created\nfor name in DATASETS:\n  uri = dataset2path(name)\n  b = tfds.builder_from_directory(builder_dir=uri)\n  split = list(b.info.splits.keys())[0]\n  b.as_dataset(split=split)\n  print('Dataset %s has size %s'%(uri, b.info.dataset_size))\n# Might require updating tensorflow datasets:\n!pip install --upgrade --force-reinstall git+https://github.com/tensorflow/datasets.git\nfor name in DATASET_NAMES:\n  print(name)\n  b = tfds.builder(name)"
        }
    ]
}