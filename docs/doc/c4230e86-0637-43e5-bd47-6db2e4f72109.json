{
    "summary": "This code installs libraries, initializes inference policies and performs inference with dummy input. It creates a dataset object for an episode and includes resizing/scaling functions, displaying images as GIF using TensorFlow. It uses a universal sentence encoder for task string embeddings, initializes policy states, performs model predictions and plotting, comparing ground truth and predicted actions over time.",
    "details": [
        {
            "comment": "This code installs required libraries, downloads and unzips a checkpoint folder, imports necessary modules, defines a function to render images as a GIF, and loads the TF model checkpoint.",
            "location": "\"/media/root/Prima/works/open_x_embodiment/docs/src/colabs/Minimal_example_for_running_inference_using_RT_1_X_TF_using_tensorflow_datasets.txt\":0-34",
            "content": "# Install required library\n# Using tfp-nightly due to https://github.com/tensorflow/probability/issues/1752\n!pip install rlds tf_agents dm-reverb[tensorflow] apache_beam tfp-nightly\n# Download zipped checkpoint folder\n!gsutil -m cp -r gs://gdm-robotics-open-x-embodiment/open_x_embodiment_and_rt_x_oss/rt_1_x_tf_trained_for_002272480_step.zip .\n# Unzip zipped checkpoint folder\n!unzip rt_1_x_tf_trained_for_002272480_step.zip\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport rlds\nfrom PIL import Image\nimport numpy as np\nfrom tf_agents.policies import py_tf_eager_policy\nimport tf_agents\nfrom tf_agents.trajectories import time_step as ts\nfrom IPython import display\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\nimport tensorflow_hub as hub\ndef as_gif(images):\n  # Render the images as the gif:\n  images[0].save('/tmp/temp.gif', save_all=True, append_images=images[1:], duration=1000, loop=0)\n  gif_bytes = open('/tmp/temp.gif','rb').read()\n  return gif_bytes\n# Load TF model checkpoint"
        },
        {
            "comment": "This code initializes a policy for running inference on a saved model, performs one step of inference using dummy input, and creates a dataset object to obtain an episode from. The saved model path is provided, and the policy state is initialized before running inference with a time step containing a zeroed observation and reward.",
            "location": "\"/media/root/Prima/works/open_x_embodiment/docs/src/colabs/Minimal_example_for_running_inference_using_RT_1_X_TF_using_tensorflow_datasets.txt\":35-61",
            "content": "# Replace saved_model_path with path to the parent folder of\n# the folder rt_1_x_tf_trained_for_002272480_step.\nsaved_model_path = 'rt_1_x_tf_trained_for_002272480_step'\ntfa_policy = py_tf_eager_policy.SavedModelPyTFEagerPolicy(\n    model_path=saved_model_path,\n    load_specs_from_pbtxt=True,\n    use_tf_function=True)\n# Perform one step of inference using dummy input\n# Obtain a dummy observation, where the features are all 0\nobservation = tf_agents.specs.zero_spec_nest(tf_agents.specs.from_spec(tfa_policy.time_step_spec.observation))\n# Construct a tf_agents time_step from the dummy observation\ntfa_time_step = ts.transition(observation, reward=np.zeros((), dtype=np.float32))\n# Initialize the state of the policy\npolicy_state = tfa_policy.get_initial_state(batch_size=1)\n# Run inference using the policy\naction = tfa_policy.action(tfa_time_step, policy_state)\n# Create a dataset object to obtain episode from\nbuilder = tfds.builder_from_directory(builder_dir='gs://gresearch/robotics/bridge/0.1.0/')\nds = builder.as_dataset(split='train[:1]')"
        },
        {
            "comment": "This code defines functions for resizing images, converting terminate_episode boolean to action tensor, and scaling actions. It also displays the number of images in an episode and renders them as a GIF. The code is part of an OpenAI Gym environment implementation using TensorFlow and other libraries.",
            "location": "\"/media/root/Prima/works/open_x_embodiment/docs/src/colabs/Minimal_example_for_running_inference_using_RT_1_X_TF_using_tensorflow_datasets.txt\":63-102",
            "content": "ds_iterator = iter(ds)\n# Obtain the steps from one episode from the dataset\nepisode = next(ds_iterator)\nsteps = episode[rlds.STEPS]\nimages = []\nfor step in steps:\n  im = Image.fromarray(np.array(step['observation']['image']))\n  images.append(im)\nprint(f'{len(images)} images')\ndisplay.Image(as_gif(images))\ndef resize(image):\n  image = tf.image.resize_with_pad(image, target_width=320, target_height=256)\n  image = tf.cast(image, tf.uint8)\n  return image\ndef terminate_bool_to_act(terminate_episode: tf.Tensor) -> tf.Tensor:\n  return tf.cond(\n      terminate_episode == tf.constant(1.0),\n      lambda: tf.constant([1, 0, 0], dtype=tf.int32),\n      lambda: tf.constant([0, 1, 0], dtype=tf.int32),\n  )\ndef rescale_action_with_bound(\n    actions: tf.Tensor,\n    low: float,\n    high: float,\n    safety_margin: float = 0,\n    post_scaling_max: float = 1.0,\n    post_scaling_min: float = -1.0,\n) -> tf.Tensor:\n  \"\"\"Formula taken from https://stats.stackexchange.com/questions/281162/scale-a-number-between-a-range.\"\"\"\n  resc_actions = (actions - low) / (high - low) * ("
        },
        {
            "comment": "The code defines three functions: `rescale_action`, `rescale_action_with_bound`, and `to_model_action`. The `rescale_action` function takes an action as input and scales its 'world_vector' and 'rotation_delta' to a specified range using `rescale_action_with_bound`. Finally, the `to_model_action` function converts a dataset action into a model action for the Bridge dataset.",
            "location": "\"/media/root/Prima/works/open_x_embodiment/docs/src/colabs/Minimal_example_for_running_inference_using_RT_1_X_TF_using_tensorflow_datasets.txt\":103-143",
            "content": "      post_scaling_max - post_scaling_min\n  ) + post_scaling_min\n  return tf.clip_by_value(\n      resc_actions,\n      post_scaling_min + safety_margin,\n      post_scaling_max - safety_margin,\n  )\ndef rescale_action(action):\n  \"\"\"Rescales action.\"\"\"\n  action['world_vector'] = rescale_action_with_bound(\n      action['world_vector'],\n      low=-0.05,\n      high=0.05,\n      safety_margin=0.01,\n      post_scaling_max=1.75,\n      post_scaling_min=-1.75,\n  )\n  action['rotation_delta'] = rescale_action_with_bound(\n      action['rotation_delta'],\n      low=-0.25,\n      high=0.25,\n      safety_margin=0.01,\n      post_scaling_max=1.4,\n      post_scaling_min=-1.4,\n  )\n  return action\ndef to_model_action(from_step):\n  \"\"\"Convert dataset action to model action. This function is specific for the Bridge dataset.\"\"\"\n  model_action = {}\n  model_action['world_vector'] = from_step['action']['world_vector']\n  model_action['terminate_episode'] = terminate_bool_to_act(\n      from_step['action']['terminate_episode']\n  )\n  model_action['rotation_delta'] = from_step['action']['rotation_delta']"
        },
        {
            "comment": "Code snippet loads a universal sentence encoder, embeds the task string using it and returns the model action by checking if the gripper is open or closed based on the dataset being used. It also scales the action before returning.",
            "location": "\"/media/root/Prima/works/open_x_embodiment/docs/src/colabs/Minimal_example_for_running_inference_using_RT_1_X_TF_using_tensorflow_datasets.txt\":145-178",
            "content": "  open_gripper = from_step['action']['open_gripper']\n  possible_values = tf.constant([True, False], dtype=tf.bool)\n  eq = tf.equal(possible_values, open_gripper)\n  assert_op = tf.Assert(tf.reduce_any(eq), [open_gripper])\n  with tf.control_dependencies([assert_op]):\n    model_action['gripper_closedness_action'] = tf.cond(\n        # for open_gripper in bridge dataset,\n        # 0 is fully closed and 1 is fully open\n        open_gripper,\n        # for Fractal data,\n        # gripper_closedness_action = -1 means opening the gripper and\n        # gripper_closedness_action = 1 means closing the gripper.\n        lambda: tf.constant([-1.0], dtype=tf.float32),\n        lambda: tf.constant([1.0], dtype=tf.float32),\n    )\n  model_action = rescale_action(model_action)\n  return model_action\nsteps = list(steps)\n# Load language model and\nembed = hub.load(\n    'https://tfhub.dev/google/universal-sentence-encoder-large/5')\n# embed the task string\nepisode_natural_language_instruction = steps[0][rlds.OBSERVATION]['natural_language_instruction'].numpy().decode()"
        },
        {
            "comment": "This code defines a function to normalize task names, extracts natural language embeddings from normalized instructions, initializes policy states for the TFA policy, and processes images and observations in a loop. It appends actions and rewards to corresponding lists, creates defaultdicts for action-value pairs over time, and performs model predictions.",
            "location": "\"/media/root/Prima/works/open_x_embodiment/docs/src/colabs/Minimal_example_for_running_inference_using_RT_1_X_TF_using_tensorflow_datasets.txt\":180-216",
            "content": "def normalize_task_name(task_name):\n  replaced = task_name.replace('_', ' ').replace('1f', ' ').replace(\n      '4f', ' ').replace('-', ' ').replace('50',\n                                           ' ').replace('55',\n                                                        ' ').replace('56', ' ')\n  return replaced.lstrip(' ').rstrip(' ')\nnatural_language_embedding = embed([normalize_task_name(episode_natural_language_instruction)])[0]\n# %%time\npolicy_state = tfa_policy.get_initial_state(batch_size=1)\ngt_actions = []\npredicted_actions = []\nimages = []\nfor step in steps:\n  image = resize(step[rlds.OBSERVATION]['image'])\n  images.append(image)\n  observation['image'] = image\n  tfa_time_step = ts.transition(observation, reward=np.zeros((), dtype=np.float32))\n  policy_step = tfa_policy.action(tfa_time_step, policy_state)\n  action = policy_step.action\n  policy_state = policy_step.state\n  predicted_actions.append(action)\n  gt_actions.append(to_model_action(step))\naction_name_to_values_over_time = defaultdict(list)\npredicted_action_name_to_values_over_time = defaultdict(list)"
        },
        {
            "comment": "This code is plotting a graph with multiple data sets. It creates a figure layout, iterates through the actions, and then appends values to two dictionaries based on action names and sub-dimensions. It updates the rcParams for font size and creates stacked images. Finally, it uses plt.subplot_mosaic() to create a figure with the specified layout.",
            "location": "\"/media/root/Prima/works/open_x_embodiment/docs/src/colabs/Minimal_example_for_running_inference_using_RT_1_X_TF_using_tensorflow_datasets.txt\":217-245",
            "content": "figure_layout = ['terminate_episode_0', 'terminate_episode_1',\n        'terminate_episode_2', 'world_vector_0', 'world_vector_1',\n        'world_vector_2', 'rotation_delta_0', 'rotation_delta_1',\n        'rotation_delta_2', 'gripper_closedness_action_0']\naction_order = ['terminate_episode', 'world_vector', 'rotation_delta', 'gripper_closedness_action']\nfor i, action in enumerate(gt_actions):\n  for action_name in action_order:\n    for action_sub_dimension in range(action[action_name].shape[0]):\n      # print(action_name, action_sub_dimension)\n      title = f'{action_name}_{action_sub_dimension}'\n      action_name_to_values_over_time[title].append(action[action_name][action_sub_dimension])\n      predicted_action_name_to_values_over_time[title].append(predicted_actions[i][action_name][action_sub_dimension])\nfigure_layout = [\n    ['image'] * len(figure_layout),\n    figure_layout\n]\nplt.rcParams.update({'font.size': 12})\nstacked = tf.concat(tf.unstack(images[::3], axis=0), 1)\nfig, axs = plt.subplot_mosaic(figure_layout)"
        },
        {
            "comment": "This code plots and compares ground truth and predicted actions over time, sets titles and x-labels, and displays an image with subsampled time data in a matplotlib figure.",
            "location": "\"/media/root/Prima/works/open_x_embodiment/docs/src/colabs/Minimal_example_for_running_inference_using_RT_1_X_TF_using_tensorflow_datasets.txt\":246-258",
            "content": "fig.set_size_inches([45, 10])\nfor i, (k, v) in enumerate(action_name_to_values_over_time.items()):\n  axs[k].plot(v, label='ground truth')\n  axs[k].plot(predicted_action_name_to_values_over_time[k], label='predicted action')\n  axs[k].set_title(k)\n  axs[k].set_xlabel('Time in one episode')\naxs['image'].imshow(stacked.numpy())\naxs['image'].set_xlabel('Time in one episode (subsampled)')\nplt.legend()"
        }
    ]
}